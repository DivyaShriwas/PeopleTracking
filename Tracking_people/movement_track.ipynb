{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Tracking_people\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Tracking_people\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Check for GPU and use it if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model from torchvision\n",
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define the video file path\n",
    "video_path = r\"cctv_feed_2.mp4\"\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get the video frame rate and size\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer\n",
    "output_file = 'output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Initialize DeepSORT with tuned parameters\n",
    "tracker = DeepSort(max_age=30, n_init=3, nms_max_overlap=1.0, max_iou_distance=0.7, nn_budget=100)\n",
    "\n",
    "# Frame selection logic\n",
    "frame_count = 0\n",
    "selected_frames = [int(fps / 3), int(fps * 2 / 3), int(fps - 1)]  # First, middle, and last frame indices in one second\n",
    "\n",
    "# Custom ID mapping\n",
    "id_mapping = {}\n",
    "next_id = 1\n",
    "\n",
    "# Initialize the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize dictionary to store trajectories\n",
    "trajectories = {}\n",
    "\n",
    "# Sectioning the video\n",
    "sections = {\n",
    "    'Sect1': ((1900, 10), (1000, 1050)),\n",
    "    'Sect2': ((750, 400), (10, 1050)),\n",
    "    'Sect3': ((900, 10), (10, 350))\n",
    "}\n",
    "\n",
    "def person_within_section(point, section_box):\n",
    "    px, py = point\n",
    "    (bx1, by1), (bx2, by2) = section_box\n",
    "    return (bx2 <= px <= bx1) and (by1 <= py <= by2)\n",
    "\n",
    "def process_frame(frame):\n",
    "    global next_id\n",
    "\n",
    "    # Convert the frame to a PIL image\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Apply the transformation to the frame\n",
    "    image = transform(pil_image)\n",
    "\n",
    "    # Add a batch dimension and move to the device (GPU)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform object detection\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "\n",
    "    # Initializing section counts\n",
    "    section_counts = {label: 0 for label in sections}\n",
    "\n",
    "    # Extract the bounding boxes, labels, and scores\n",
    "    boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "    labels = outputs[0]['labels'].cpu().numpy()\n",
    "    scores = outputs[0]['scores'].cpu().numpy()\n",
    "\n",
    "    # Prepare the detections for DeepSORT\n",
    "    detections = []\n",
    "    for i in range(len(boxes)):\n",
    "        if scores[i] >= 0.5 and labels[i] == 1:  # 1 is the label for person\n",
    "            box = boxes[i]\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            detections.append((np.array([x1, y1, width, height]), scores[i]))\n",
    "\n",
    "            centre_point = (x1 + width // 2, y1 + height // 2)\n",
    "            cv2.circle(frame, centre_point, radius=5, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "            for lab, ((top_left_x, top_left_y), (bottom_right_x, bottom_right_y)) in sections.items():\n",
    "                cv2.rectangle(frame, (top_left_x, top_left_y), (bottom_right_x, bottom_right_y), color=(255, 0, 0), thickness=2)\n",
    "                if person_within_section(centre_point, ((top_left_x, top_left_y), (bottom_right_x, bottom_right_y))):\n",
    "                    section_counts[lab] += 1\n",
    "\n",
    "    # Displaying sections along with section counts\n",
    "    for lab, ((top_left_x, top_left_y), (bottom_right_x, bottom_right_y)) in sections.items():\n",
    "        cv2.putText(frame, f'{lab} Count: {section_counts[lab]}', (top_left_x - 250, top_left_y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Update the tracker\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # Draw bounding boxes and unique IDs, update trajectories\n",
    "    person_count = 0\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue\n",
    "        person_count += 1\n",
    "        bbox = track.to_tlbr()  # Get the bounding box coordinates\n",
    "        track_id = track.track_id  # Get the unique ID\n",
    "\n",
    "        # Custom incremental ID mapping\n",
    "        if track_id not in id_mapping:\n",
    "            id_mapping[track_id] = next_id\n",
    "            next_id += 1\n",
    "\n",
    "        custom_id = id_mapping[track_id]\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'ID: {custom_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Update trajectory\n",
    "        centre_point = (x1 + (x2 - x1) // 2, y1 + (y2 - y1) // 2)\n",
    "        if custom_id not in trajectories:\n",
    "            trajectories[custom_id] = []\n",
    "        trajectories[custom_id].append(centre_point)\n",
    "\n",
    "    # Draw all trajectories\n",
    "    for custom_id, points in trajectories.items():\n",
    "        if points:\n",
    "            # Mark the starting point\n",
    "            start_point = points[0]\n",
    "            cv2.circle(frame, start_point, radius=8, color=(255, 0, 0), thickness=-1)\n",
    "        for i in range(1, len(points)):\n",
    "            cv2.line(frame, points[i - 1], points[i], (0, 0, 255), 2)\n",
    "\n",
    "    # Display the person count\n",
    "    cv2.putText(frame, f'Total Persons: {person_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Main processing loop\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES)) - 1\n",
    "    if frame_number % int(fps) in selected_frames:\n",
    "        processed_frame = process_frame(frame)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        out.write(processed_frame)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
